{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### © Habibi Group, Fall 2024\n",
    "This the second model for the project. It uses a custom build vectorizer to make sparse vectors for each sentence and then uses cosine distance (dot product) as the nearess measure. The model is trained on the training data and then tested on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*THIS IS THE COMBINED DATA FLAVOR 2*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the dataset for the *Naive Bayes* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>local_id</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://urdu.arynews.tv/car-sales-in-pakistan/</td>\n",
       "      <td>پاکستان میں گاڑیوں کی فروخت میں بڑا اضافہ</td>\n",
       "      <td>ملکی آٹو سیکٹر سے زبردست خبر آگئی۔ پاکستان می...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>https://urdu.arynews.tv/gold-rates-in-pakistan-3/</td>\n",
       "      <td>پاکستان میں سونے کی قیمت آج کتنی کم ہوئی؟</td>\n",
       "      <td>کراچی: کاروباری ہفتے کے پہلے روز سونے کی قیمت ...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>https://urdu.arynews.tv/cotton-production-cott...</td>\n",
       "      <td>امریکا سے معیاری روئی کی درآمد بڑھ گئی</td>\n",
       "      <td>کراچی: پاکستان میں کپاس کی پیداوار میں کمی کے ...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>https://urdu.arynews.tv/psx-today-11-nov/</td>\n",
       "      <td>پاکستان اسٹاک ایکسچینج میں نئی تاریخ رقم</td>\n",
       "      <td>پاکستان اسٹاک ایکسچینج نے ایک اور سنگ میل عبور...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>https://urdu.arynews.tv/ghee-and-cooking-oil-p...</td>\n",
       "      <td>عوام کے لیے نئی مشکل : گھی اور کوکنگ آئل کی قی...</td>\n",
       "      <td>لاہور : گھی اور کوکنگ آئل کی قیمتوں میں ایک با...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  local_id                                               link  \\\n",
       "0   1         1     https://urdu.arynews.tv/car-sales-in-pakistan/   \n",
       "1   2         2  https://urdu.arynews.tv/gold-rates-in-pakistan-3/   \n",
       "2   3         5  https://urdu.arynews.tv/cotton-production-cott...   \n",
       "3   4         3          https://urdu.arynews.tv/psx-today-11-nov/   \n",
       "4   5         4  https://urdu.arynews.tv/ghee-and-cooking-oil-p...   \n",
       "\n",
       "                                               title  \\\n",
       "0          پاکستان میں گاڑیوں کی فروخت میں بڑا اضافہ   \n",
       "1         پاکستان میں سونے کی قیمت آج کتنی کم ہوئی؟   \n",
       "2             امریکا سے معیاری روئی کی درآمد بڑھ گئی   \n",
       "3           پاکستان اسٹاک ایکسچینج میں نئی تاریخ رقم   \n",
       "4  عوام کے لیے نئی مشکل : گھی اور کوکنگ آئل کی قی...   \n",
       "\n",
       "                                             content gold_label  \n",
       "0  ملکی آٹو سیکٹر سے زبردست خبر آگئی۔ پاکستان می...   Business  \n",
       "1  کراچی: کاروباری ہفتے کے پہلے روز سونے کی قیمت ...   Business  \n",
       "2  کراچی: پاکستان میں کپاس کی پیداوار میں کمی کے ...   Business  \n",
       "3  پاکستان اسٹاک ایکسچینج نے ایک اور سنگ میل عبور...   Business  \n",
       "4  لاہور : گھی اور کوکنگ آئل کی قیمتوں میں ایک با...   Business  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading data\n",
    "df = pd.read_csv('../combined_data/dataset.csv')\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>local_id</th>\n",
       "      <th>link</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>https://urdu.arynews.tv/car-sales-in-pakistan/</td>\n",
       "      <td>پاکستان میں گاڑیوں کی فروخت میں بڑا اضافہ</td>\n",
       "      <td>ملک آٹ سیکٹر س زبردست خبر آگئی۔ پاکستان گاڑی ...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>https://urdu.arynews.tv/gold-rates-in-pakistan-3/</td>\n",
       "      <td>پاکستان میں سونے کی قیمت آج کتنی کم ہوئی؟</td>\n",
       "      <td>کراچ کاروبار ہفت پہل روز سون قیمت رجحان رہا۔ پ...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>https://urdu.arynews.tv/cotton-production-cott...</td>\n",
       "      <td>امریکا سے معیاری روئی کی درآمد بڑھ گئی</td>\n",
       "      <td>کراچ پاکستان کپاس پیداوار باعث اسپننگ مل س معی...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>https://urdu.arynews.tv/psx-today-11-nov/</td>\n",
       "      <td>پاکستان اسٹاک ایکسچینج میں نئی تاریخ رقم</td>\n",
       "      <td>پاکستان اسٹاک ایکسچینج ن میل عبور لیا۔ کاروبار...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>https://urdu.arynews.tv/ghee-and-cooking-oil-p...</td>\n",
       "      <td>عوام کے لیے نئی مشکل : گھی اور کوکنگ آئل کی قی...</td>\n",
       "      <td>لاہور کوکنگ آئل قیمت اضاف ہوا، قمیت س تجاوز کر...</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  local_id                                               link  \\\n",
       "0   1         1     https://urdu.arynews.tv/car-sales-in-pakistan/   \n",
       "1   2         2  https://urdu.arynews.tv/gold-rates-in-pakistan-3/   \n",
       "2   3         5  https://urdu.arynews.tv/cotton-production-cott...   \n",
       "3   4         3          https://urdu.arynews.tv/psx-today-11-nov/   \n",
       "4   5         4  https://urdu.arynews.tv/ghee-and-cooking-oil-p...   \n",
       "\n",
       "                                               title  \\\n",
       "0          پاکستان میں گاڑیوں کی فروخت میں بڑا اضافہ   \n",
       "1         پاکستان میں سونے کی قیمت آج کتنی کم ہوئی؟   \n",
       "2             امریکا سے معیاری روئی کی درآمد بڑھ گئی   \n",
       "3           پاکستان اسٹاک ایکسچینج میں نئی تاریخ رقم   \n",
       "4  عوام کے لیے نئی مشکل : گھی اور کوکنگ آئل کی قی...   \n",
       "\n",
       "                                             content gold_label  \n",
       "0  ملک آٹ سیکٹر س زبردست خبر آگئی۔ پاکستان گاڑی ...   Business  \n",
       "1  کراچ کاروبار ہفت پہل روز سون قیمت رجحان رہا۔ پ...   Business  \n",
       "2  کراچ پاکستان کپاس پیداوار باعث اسپننگ مل س معی...   Business  \n",
       "3  پاکستان اسٹاک ایکسچینج ن میل عبور لیا۔ کاروبار...   Business  \n",
       "4  لاہور کوکنگ آئل قیمت اضاف ہوا، قمیت س تجاوز کر...   Business  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocessing the data\n",
    "# Add this function to perform stemming\n",
    "def simple_urdu_stemmer(word):\n",
    "    suffixes = ['یں', 'اں', 'وں', 'یں', 'ہاں', 'ی', 'ے', 'و', 'ہ']\n",
    "    for suffix in suffixes:\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "# Loading Urdu stopwords from the json file\n",
    "with open('../data/kaggle_stopwords.json', 'r', encoding='utf-8') as file:\n",
    "    urdu_stopwords = set(json.load(file).keys())\n",
    "\n",
    "#Loading Shanzae Stopwords\n",
    "with open('../data/shanzae/stopwords.json', 'r', encoding='utf-8') as file:\n",
    "    shanzae_stopwords = set(json.load(file).keys())\n",
    "\n",
    "#Loading Yamsheen Stopwords\n",
    "with open('../data/yamsheen/stopwords.json', 'r', encoding='utf-8') as file:\n",
    "    yamsheen_stopwords = set(json.load(file).keys())\n",
    "\n",
    "# Function to clean our Urdu sentences\n",
    "def clean_content(text, stopwords):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords)\n",
    "    text = ' '.join(word for word in text.split() if word not in shanzae_stopwords)\n",
    "    text = ' '.join(word for word in text.split() if word not in yamsheen_stopwords)\n",
    "    text = text.lower()\n",
    "    text = ' '.join(simple_urdu_stemmer(word) for word in text.split())\n",
    "    return text\n",
    "\n",
    "df['content'] = df['content'].apply(lambda x: clean_content(x, urdu_stopwords))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing TF-IDF Vectorizer\n",
    "def compute_tfidf(corpus):\n",
    "    tf = defaultdict(Counter)\n",
    "    df = Counter()\n",
    "    N = len(corpus)\n",
    "    \n",
    "    for doc_id, doc in enumerate(corpus):\n",
    "        tokens = doc.split()\n",
    "        tf[doc_id].update(tokens)\n",
    "        for token in set(tokens):\n",
    "            df[token] += 1\n",
    "    \n",
    "    tfidf = defaultdict(dict)\n",
    "    for doc_id, term_freqs in tf.items():\n",
    "        for term, freq in term_freqs.items():\n",
    "            tfidf[doc_id][term] = (freq / len(term_freqs)) * math.log(N / (df[term] + 1))\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Naive Bayes Classifier\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.prior_probs = {}\n",
    "        self.ngram_counts = {}\n",
    "        self.total_ngrams_per_class = {}\n",
    "        self.vocabulary = set()\n",
    "\n",
    "    def train(self, data, n=1):\n",
    "        class_counts = data['gold_label'].value_counts()\n",
    "        total_documents = len(data)\n",
    "        self.prior_probs = {cls: count / total_documents for cls, count in class_counts.items()}\n",
    "\n",
    "        self.ngram_counts = {cls: defaultdict(int) for cls in class_counts.index}\n",
    "        self.total_ngrams_per_class = {cls: 0 for cls in class_counts.index}\n",
    "\n",
    "        for index, row in data.iterrows():\n",
    "            cls = row['gold_label']\n",
    "            tokens = row['content'].split()\n",
    "            ngrams = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "            self.vocabulary.update(ngrams)\n",
    "            for ngram in ngrams:\n",
    "                self.ngram_counts[cls][ngram] += 1\n",
    "                self.total_ngrams_per_class[cls] += 1\n",
    "\n",
    "    def predict(self, data, n=1):\n",
    "        predictions = []\n",
    "        vocab_size = len(self.vocabulary)\n",
    "\n",
    "        for index, row in data.iterrows():\n",
    "            tokens = row['content'].split()\n",
    "            ngrams = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "            class_scores = {}\n",
    "\n",
    "            for cls in self.prior_probs:\n",
    "                log_prob = 0\n",
    "                for ngram in ngrams:\n",
    "                    count = self.ngram_counts[cls].get(ngram, 0) + 1  # Laplace smoothing\n",
    "                    total = self.total_ngrams_per_class[cls] + vocab_size\n",
    "                    log_prob += math.log(count / total)\n",
    "                class_scores[cls] = math.log(self.prior_probs[cls]) + log_prob\n",
    "\n",
    "            predicted_class = max(class_scores, key=class_scores.get)\n",
    "            predictions.append(predicted_class)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model.<br>\n",
    "*We are using unigrams for **Naive Bayes** because unigrams gives out the best accuracies in our testing of the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset: 0.8782\n",
      "\n",
      "Classification Report on test dataset:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Business       0.84      0.89      0.86       246\n",
      "     Entertainment       0.91      0.92      0.91       284\n",
      "     International       0.82      0.80      0.81       299\n",
      "Science-Technology       0.86      0.83      0.85       311\n",
      "            Sports       0.96      0.95      0.96       305\n",
      "\n",
      "          accuracy                           0.88      1445\n",
      "         macro avg       0.88      0.88      0.88      1445\n",
      "      weighted avg       0.88      0.88      0.88      1445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "classifier = NaiveBayesClassifier()\n",
    "n = 1  # Use unigrams\n",
    "classifier.train(train_data, n)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = classifier.predict(test_data, n)\n",
    "\n",
    "# Evaluate the performance\n",
    "y_val = test_data['gold_label'].values\n",
    "accuracy = accuracy_score(y_val, predictions)\n",
    "report = classification_report(y_val, predictions)\n",
    "\n",
    "print(f\"Accuracy on test dataset: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report on test dataset:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing on Externally Source Data *(to mimic the real-world scenario)*\n",
    "\n",
    "- The test is on `DAWN` dataset, which follows a similar distribution as our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on new dataset: 0.8974358974358975\n",
      "\n",
      "Classification Report on new dataset:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Business       0.96      0.86      0.91        63\n",
      "     Entertainment       0.00      0.00      0.00         3\n",
      "     International       0.90      0.97      0.93       149\n",
      "Science-Technology       0.85      0.58      0.69        19\n",
      "            Sports       0.00      0.00      0.00         0\n",
      "\n",
      "          accuracy                           0.90       234\n",
      "         macro avg       0.54      0.48      0.51       234\n",
      "      weighted avg       0.90      0.90      0.89       234\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ahmed\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ahmed\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Load the new dataset\n",
    "new_df = pd.read_csv('../data/dawn_dataset_c.csv')\n",
    "\n",
    "# Preprocess the content\n",
    "new_df['content'] = new_df['content'].apply(lambda x: clean_content(x, urdu_stopwords))\n",
    "\n",
    "# Predict using the trained Naive Bayes classifier\n",
    "predictions = classifier.predict(new_df, n)\n",
    "\n",
    "# If the new dataset has labels, evaluate the performance\n",
    "if 'gold_label' in new_df.columns:\n",
    "    y_true = new_df['gold_label'].values\n",
    "    accuracy = accuracy_score(y_true, predictions)\n",
    "    report = classification_report(y_true, predictions)\n",
    "    print(\"Accuracy on new dataset:\", accuracy)\n",
    "    print(\"\\nClassification Report on new dataset:\")\n",
    "    print(report)\n",
    "else:\n",
    "    # Add predictions to the dataframe\n",
    "    new_df['predicted_labels'] = predictions\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    new_df.to_csv('./data/dawn_dataset_predictions.csv', index=False)\n",
    "\n",
    "    # Display the predictions\n",
    "    print(new_df[['content', 'predicted_labels']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The test is on `BBC` dataset, which follows a different distribution as our training set with long articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on new dataset: 0.8458188153310104\n",
      "\n",
      "Classification Report on new dataset:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          Business       0.91      0.86      0.88       222\n",
      "     Entertainment       0.85      0.89      0.87       240\n",
      "     International       0.75      0.88      0.81       208\n",
      "Science-Technology       0.76      0.67      0.71       239\n",
      "            Sports       0.97      0.94      0.95       239\n",
      "\n",
      "          accuracy                           0.85      1148\n",
      "         macro avg       0.85      0.85      0.84      1148\n",
      "      weighted avg       0.85      0.85      0.85      1148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the new dataset\n",
    "new_df = pd.read_csv('../data/bbc_dataset_c.csv')\n",
    "\n",
    "# Preprocess the content\n",
    "new_df['content'] = new_df['content'].apply(lambda x: clean_content(x, urdu_stopwords))\n",
    "\n",
    "# Predict using the trained Naive Bayes classifier\n",
    "predictions = classifier.predict(new_df, n)\n",
    "\n",
    "# If the new dataset has labels, evaluate the performance\n",
    "if 'gold_label' in new_df.columns:\n",
    "    y_true = new_df['gold_label'].values\n",
    "    accuracy = accuracy_score(y_true, predictions)\n",
    "    report = classification_report(y_true, predictions)\n",
    "    print(\"Accuracy on new dataset:\", accuracy)\n",
    "    print(\"\\nClassification Report on new dataset:\")\n",
    "    print(report)\n",
    "else:\n",
    "    # Add predictions to the dataframe\n",
    "    new_df['predicted_labels'] = predictions\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    new_df.to_csv('./data/dawn_dataset_predictions.csv', index=False)\n",
    "\n",
    "    # Display the predictions\n",
    "    print(new_df[['content', 'predicted_labels']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Thank you for bearing through this end*.<br>\n",
    "For testing your dataset, please change one of the above *External Test Datasets* to your dataset and run the code. The notebook will automatically test the model on the new dataset. Please ensure that the file direcotry is correct and the dataset is in the same format as the training and testing datasets. See *Testing your dataset* section in the report for more details.\n",
    "###### (c) Habibi Group, Fall 2024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
